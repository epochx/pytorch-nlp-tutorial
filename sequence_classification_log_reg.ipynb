{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFc19f5WH4og"
   },
   "source": [
    " # Logistic Regression with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1X8Dv3oEiIL7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kNiTAHZalfce"
   },
   "source": [
    "## Brief Pytorch API summary\n",
    "- The interface is very similar to numpy, operations are based on Tensors, which are roughly similar to numpy ndarrays.\n",
    "- Most operators return a new tensor after computing, but some can be applied 'in-place', meaning that the operations are performed over the object. These functions generally have an underscore in their name.   \n",
    "- Simple operations, such as addition and product, are overloaded into Python syntax.\n",
    "- More complex functions are accessible using the API, having [pytorch.org/docs](http://pytorch.org/docs) in the background when developing is always recommended.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XZntaQ38iP6Q",
    "outputId": "ed89fca0-90a4-49f9-e587-ded10a6cba93"
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1,2], [3,4]])\n",
    "print(x)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "RxPh4KwMmoaA",
    "outputId": "4a865dbe-006a-45ec-8f27-b76cac66238c"
   },
   "outputs": [],
   "source": [
    "print(x+1)\n",
    "x.add_(1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcSw_EtYp6CK"
   },
   "source": [
    "- Tensors can be transposed, reshaped and manipulated according to our needs. This is mainly accomplished using the `transpose()` and `view()` functions. \n",
    "- These reshapings are generally needed when we use the Pytorch API for neural nets, which makes certain assumptions about the shape of the input data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "bUunRQMQp1VK",
    "outputId": "1c7faffc-7f76-4104-f999-0cc7d4df4080"
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(x.transpose(0, 1))\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQ2kYy9Lrkn4"
   },
   "source": [
    "- the `torch.Tensor` object implicitly represents one node in the computational graph, meaning that all operations we perform over `Tensor` objects will be recorded, unless we state otherwise. In this way,  we can later go through the graph created implicitly and use it for backpropagation. \n",
    "- Every `Tensor` object contains a`.grad` attribute holding the current value of the gradient, if any. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loaYTwer8enE"
   },
   "source": [
    "- By simply calling the `.to(device)` function on any Tensor or Module, we can easily move our objects to the GPU/CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "4WU6okWI8d82",
    "outputId": "c547234a-8308-47d5-b537-5792233af2ad"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "x.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHjeu_ewtiFG"
   },
   "source": [
    "- Nodes in the graph should implement both the `forward()` and `backward()` functions, making them suitable to use when training models using backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7naog-jplwhY"
   },
   "source": [
    "## Loading and understanding the SemEval 2018 Task 3 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Aodz-PUuOGIG",
    "outputId": "cb50a25f-9328-4c04-e238-3cd2a9b5b85c"
   },
   "outputs": [],
   "source": [
    "# download the SemEval 2018 Task 3 Dataset\n",
    "# the task is binary classification: ironic or not ironic tweet\n",
    "! wget https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/train/SemEval2018-T3-train-taskA.txt\n",
    "! wget https://raw.githubusercontent.com/Cyvhee/SemEval2018-Task3/master/datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSWoOty0-AHo"
   },
   "outputs": [],
   "source": [
    "# let's set some parameters\n",
    "\n",
    "train_path = 'SemEval2018-T3-train-taskA.txt' \n",
    "test_path = 'SemEval2018-T3_gold_test_taskA_emoji.txt'\n",
    "\n",
    "batch_size = 32\n",
    "max_len = 300\n",
    "min_count = 0\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiNLFznj-BpW"
   },
   "source": [
    "## Load the dataset\n",
    "- 3834 train tweets\n",
    "- 784 test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lwiudOkMMDj"
   },
   "outputs": [],
   "source": [
    "Sentence = namedtuple('Sentence', ['index', 'tokens', 'label'])\n",
    "\n",
    "def read_semeval_2018_task_3_dataset(dataset_file_path):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    with open(dataset_file_path) as f:\n",
    "        # skip header\n",
    "        f.readline()\n",
    "        for line in f.readlines(): \n",
    "            if line:\n",
    "                index, label, text = line.strip().split('\\t')\n",
    "                sentence = Sentence(index, text.split(), label)\n",
    "                sentences.append(sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "oS7QexBIMPOO",
    "outputId": "a012e729-ea27-4d9d-985a-b0e9e5abd715"
   },
   "outputs": [],
   "source": [
    "train_examples = read_semeval_2018_task_3_dataset(train_path)\n",
    "test_examples = read_semeval_2018_task_3_dataset(test_path)\n",
    "\n",
    "print(len(train_examples))\n",
    "print(len(test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D4PM-mgOdbf"
   },
   "source": [
    "## Mapping our words to unique identifiers: the Vocabulary object\n",
    "- We will create an object to manage a mapping between words (or more generally tokens) and unique indices. \n",
    "- There are a few special symbols that we will be adding to handle special cases.\n",
    "  - The first key special case is the `UNK` token, wich will represent all tokens that we do not have in our vocabulary. This is needed as we will build our vocabulary only using the training examples, and during validation or testing (or if we deploy our model in production) we may encounter new words that also need to be represented somehow.\n",
    "  - The `PAD` token, which we will use to create even-sized batches of sentences of different length when using RNNs. \n",
    "  - The beginning-of-sentence or `BOS` token, which we may use to denote the beginning of a sentence in some special cases\n",
    "  - The end-of-sentence or `EOS` token, which as in the previous case is useful for certain tasks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_r953s9HOWCN"
   },
   "outputs": [],
   "source": [
    "# Define the string of special tokens we will need \n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "\n",
    "class VocabItem:\n",
    "\n",
    "    def __init__(self, string, hash=None):\n",
    "        \"\"\"\n",
    "        Our token object, representing a term in our vocabulary.\n",
    "        \"\"\"\n",
    "        self.string = string\n",
    "        self.count = 0\n",
    "        self.hash = hash\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        For pretty-printing of our object\n",
    "        \"\"\"\n",
    "        return 'VocabItem({})'.format(self.string)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        For pretty-printing of our object\n",
    "        \"\"\"\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_count=0,\n",
    "        no_unk=False,\n",
    "        add_padding=False,\n",
    "        add_bos=False,\n",
    "        add_eos=False,\n",
    "        unk=None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        :param min_count: The minimum frequency count threshold for a token\n",
    "                          to be added to our mapping. Only useful if\n",
    "                          the unk parameter is None.\n",
    "\n",
    "        :param add_padding: If we should add the special `PAD` token.\n",
    "\n",
    "        :param add_bos: If we should add the special `BOS` token.\n",
    "\n",
    "        :param add_eos: If we should add the special `EOS` token.\n",
    "\n",
    "        :param no_unk: If we should not add the `UNK` token to our Vocab.\n",
    "\n",
    "        :param unk: A string with the unknown token, in case our\n",
    "                    sentences have already been processed for this,\n",
    "                    or `None` to use our default `UNK` token.\n",
    "        \"\"\"\n",
    "\n",
    "        self.no_unk = no_unk\n",
    "        self.vocab_items = []\n",
    "        self.vocab_hash = {}\n",
    "        self.word_count = 0\n",
    "        self.special_tokens = []\n",
    "        self.min_count = min_count\n",
    "        self.add_padding = add_padding\n",
    "        self.add_bos = add_bos\n",
    "        self.add_eos = add_eos\n",
    "        self.unk = unk\n",
    "\n",
    "        self.UNK = None\n",
    "        self.PAD = None\n",
    "        self.BOS = None\n",
    "        self.EOS = None\n",
    "\n",
    "        self.index2token = []\n",
    "        self.token2index = {}\n",
    "\n",
    "        self.finished = False\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        if self.finished:\n",
    "            raise RuntimeError('Vocabulary is finished')\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in self.vocab_hash:\n",
    "                self.vocab_hash[token] = len(self.vocab_items)\n",
    "                self.vocab_items.append(VocabItem(token))\n",
    "\n",
    "            self.vocab_items[self.vocab_hash[token]].count += 1\n",
    "            self.word_count += 1\n",
    "\n",
    "    def finish(self):\n",
    "\n",
    "        token2index = self.token2index\n",
    "        index2token = self.index2token\n",
    "\n",
    "        tmp = []\n",
    "\n",
    "        if not self.no_unk:\n",
    "\n",
    "            # we add/handle the special `UNK` token\n",
    "            # and set it to have index 0 in our mapping\n",
    "            if self.unk:\n",
    "                self.UNK = VocabItem(self.unk, hash=0)\n",
    "                self.UNK.count = self.vocab_items[self.vocab_hash[self.unk]].count\n",
    "                index2token.append(self.UNK)\n",
    "                self.special_tokens.append(self.UNK)\n",
    "\n",
    "                for token in self.vocab_items:\n",
    "                    if token.string != self.unk:\n",
    "                        tmp.append(token)\n",
    "\n",
    "            else:\n",
    "                self.UNK = VocabItem(UNK, hash=0)\n",
    "                index2token.append(self.UNK)\n",
    "                self.special_tokens.append(self.UNK)\n",
    "\n",
    "                for token in self.vocab_items:\n",
    "                    if token.count <= self.min_count:\n",
    "                        self.UNK.count += token.count\n",
    "                    else:\n",
    "                        tmp.append(token)\n",
    "        else:\n",
    "            for token in self.vocab_items:\n",
    "                tmp.append(token)\n",
    "\n",
    "        # we sort our vocab. items by frequency\n",
    "        # so for the same corpus, the indices of our words\n",
    "        # are always the same\n",
    "        tmp.sort(key=lambda token: token.count, reverse=True)\n",
    "\n",
    "        # we always add our additional special tokens\n",
    "        # at the end of our mapping\n",
    "        if self.add_bos:\n",
    "            self.BOS = VocabItem(BOS)\n",
    "            tmp.append(self.BOS)\n",
    "            self.special_tokens.append(self.BOS)\n",
    "\n",
    "        if self.add_eos:\n",
    "            self.EOS = VocabItem(EOS)\n",
    "            tmp.append(self.EOS)\n",
    "            self.special_tokens.append(self.EOS)\n",
    "\n",
    "        if self.add_padding:\n",
    "            self.PAD = VocabItem(PAD)\n",
    "            tmp.append(self.PAD)\n",
    "            self.special_tokens.append(self.PAD)\n",
    "\n",
    "        index2token += tmp\n",
    "\n",
    "        # we update the vocab_hash for each\n",
    "        # VocabItem object in our list\n",
    "        # based on their frequency\n",
    "        for i, token in enumerate(self.index2token):\n",
    "            token2index[token.string] = i\n",
    "            token.hash = i\n",
    "\n",
    "        self.index2token = index2token\n",
    "        self.token2index = token2index\n",
    "\n",
    "        if not self.no_unk:\n",
    "            print('Unknown vocab size:', self.UNK.count)\n",
    "\n",
    "        print('Vocab size: %d' % len(self))\n",
    "\n",
    "        self.finished = True\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.index2token[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index2token)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.index2token)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.token2index\n",
    "\n",
    "    def tokens2indices(self, tokens, add_bos=False, add_eos=False):\n",
    "        \"\"\"\n",
    "        Returns a list of mapping indices by processing the given string\n",
    "        with our `tokenizer` and `token_function`, and defaulting to our\n",
    "        special `UNK` token whenever we found an unseen term.\n",
    "\n",
    "        :param string: A sentence string we wish to map into our vocabulary.\n",
    "\n",
    "        :param add_bos: If we should add the `BOS` at the beginning.\n",
    "\n",
    "        :param add_eos: If we should add the `EOS` at the end.\n",
    "\n",
    "        :return: A list of ints, with the indices of each token in the\n",
    "                 given string.\n",
    "        \"\"\"\n",
    "        string_seq = []\n",
    "        if add_bos:\n",
    "            string_seq.append(self.BOS.hash)\n",
    "        for token in tokens:\n",
    "            if self.no_unk:\n",
    "                string_seq.append(self.token2index[token])\n",
    "            else:\n",
    "                string_seq.append(self.token2index.get(token, self.UNK.hash))\n",
    "        if add_eos:\n",
    "            string_seq.append(self.EOS.hash)\n",
    "        return string_seq\n",
    "\n",
    "    def indices2tokens(self, indices, ignore_ids=()):\n",
    "        \"\"\"\n",
    "        Returns a list of strings by mapping back every index to our\n",
    "        vocabulary.\n",
    "\n",
    "        :param indices: A list of ints.\n",
    "\n",
    "        :param ignore_ids: An itereable with indices to ignore, meaning\n",
    "                           that we will not look for them in our mapping.\n",
    "\n",
    "        :return: A list of strings.\n",
    "\n",
    "        Will raise a KeyException whenever we pass an index that we\n",
    "        do not have in our mapping, except when provided with `ignore_ids`.\n",
    "\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in indices:\n",
    "            if idx in ignore_ids:\n",
    "                continue\n",
    "            tokens.append(self.index2token[idx].string)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8CO6dkzBCJJ"
   },
   "source": [
    "- Now we can instance our vocabulary objects and add the data.\n",
    "- We will use one vocabulary for the input data (the sentences), and another vocabulary object for the output data, the class labels. In this way our code is generic and should work out-of-the-box for any number of output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tYV1qg5pOoKi"
   },
   "outputs": [],
   "source": [
    "# for the input vocabulary\n",
    "# we add the `UNK` special token to handle unseen data\n",
    "# we do not the padding, so we skip it (more on this below) \n",
    "src_vocab = Vocab(min_count=0, add_padding=False)\n",
    "\n",
    "# for the output vocabulary\n",
    "# we do not need the `UNK` token (we do not want an UNK class)\n",
    "# we do not the padding either (more on this below) \n",
    "tgt_vocab = Vocab(no_unk=True, add_padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "x6-bIJesOr0V",
    "outputId": "b6b9b6d4-bc40-449a-a885-227b7ea00c8b"
   },
   "outputs": [],
   "source": [
    "for sentence in train_examples:\n",
    "    src_vocab.add_tokens(sentence.tokens[:max_len])\n",
    "    tgt_vocab.add_tokens([sentence.label])\n",
    "\n",
    "src_vocab.finish()\n",
    "tgt_vocab.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Xs2TIvN8CLuZ",
    "outputId": "329925ab-9d3a-4c8a-8627-fa9d84c9879d"
   },
   "outputs": [],
   "source": [
    "src_vocab.tokens2indices('the movie was bad'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcx736J9IVg5"
   },
   "outputs": [],
   "source": [
    "Vocabs = namedtuple('Vocabs', ['src', 'tgt'])\n",
    "vocabs = Vocabs(src_vocab, tgt_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62rtkobECOxO"
   },
   "source": [
    "## Representing words using one-hot vectors\n",
    "- The building block for classic NLP in terms of representing words is the one-hot vector. \n",
    "- One-hot vectors are sparse vectors whose dimension is equivalent to the size of the vocabulary. To create the vector representation of a word we start a with a vector of zeros and simply put a 1 at the index corresponding to that word, according to our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RDCN5gHL4yp"
   },
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    input_size = len(labels)\n",
    "    labels = np.array(labels)\n",
    "    matrix = np.zeros((input_size, num_classes), dtype=np.float32)    \n",
    "    matrix[np.arange(input_size), labels] = 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fa5RDF8FUoN"
   },
   "source": [
    "## The Batch object\n",
    " - To easily access all the data in a batch, let's create a special Batch object that will give us access to all the information we may require during training. \n",
    " - This object will work like a dictionary, but it will also allow us to access each component using an attribute with the same name.\n",
    "  The main principle is that this dictionary-like batch will hold `numpy` objects as values, and that after calling the `to_torch_()` function, they will be turned into `pytorch` objects and moved to the corresponding provided device. In this way, we know that all our elements inside the batch object are in the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Batch, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        self._is_torch = False\n",
    "\n",
    "    def to_torch_(self, device):\n",
    "        self._is_torch = False\n",
    "        for key in self.keys():\n",
    "            value = self[key]\n",
    "            if isinstance(value, np.ndarray):\n",
    "                self[key] = torch.from_numpy(value).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qAaUTEYkDAMM"
   },
   "source": [
    "## The BatchBuilder object\n",
    "- Finally, let's create an object to help us transform our text data into tensors with information that can be fed into our model. This object will do all the heavy-lifting, turning our string examples into our batch objects, which PyTorch can later handle.\n",
    "- We will combine this object with the `DataLoader` util from `pytorch`, using as a function for the [`collate_fn` parameter](https://pytorch.org/docs/stable/data.html#working-with-collate-fn), which allows us to provide a custom function to create this funcion. In our case, this is achieved by implementing the `__call__` function in the `BatchBuilder` object, which will esentally turn the [instance into a function](https://docs.python.org/3/reference/datamodel.html#emulating-callable-objects).\n",
    "- In this case, to represent each input sentence in a batch we will use the **sum of its one-hot vectors**, which is the de facto input for logistic regressions. There is no need for padding since each sentence will be compressed into a vector that is the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionBatchBuilder(object):\n",
    "    # Because the `__call__` function needs to only recieve \n",
    "    # one parameter (due to restrictions of the `DataLoader`\n",
    "    # we can use the constructor we can pass any additional\n",
    "    # inputs we may require when building our batches\n",
    "    def __init__(self, vocabs, max_len=None):\n",
    "        self.vocabs = vocabs\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # This will the \"unction called by the `DataLoader` object\n",
    "    # that only accepts the `examples` parameter\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        ids_batch = [int(sentence.index) for sentence in examples]\n",
    "\n",
    "        src_examples = [\n",
    "            self.vocabs.src.tokens2indices(sentence.tokens[: self.max_len])\n",
    "            for sentence in examples\n",
    "        ]\n",
    "\n",
    "        tgt_examples = [\n",
    "            self.vocabs.tgt.token2index[sentence.label]\n",
    "            for sentence in examples\n",
    "        ]\n",
    "\n",
    "        src_examples_one_hot = [\n",
    "            one_hot(src_example, len(self.vocabs.src))\n",
    "            for src_example in src_examples\n",
    "        ]\n",
    "\n",
    "        src_batch = np.vstack(\n",
    "            [item.sum(0) for item in src_examples_one_hot]\n",
    "        )\n",
    "\n",
    "        tgt_batch = np.asarray(tgt_examples, dtype=np.int64)\n",
    "        \n",
    "        # we return our Batch \"custom object\"\n",
    "        return Batch(\n",
    "            indices=ids_batch,\n",
    "            src=src_batch,\n",
    "            tgt=tgt_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can just instance our `BatchBuilderObject` and provide it to the `DataLoader` as a `collate_fn`parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dakuSbok3Vv"
   },
   "outputs": [],
   "source": [
    "batch_builder = LogisticRegressionBatchBuilder(\n",
    "    vocabs,\n",
    "    max_len=max_len\n",
    ")\n",
    "\n",
    "train_batches = DataLoader(\n",
    "    train_examples,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_builder,\n",
    ")\n",
    "\n",
    "test_batches = DataLoader(\n",
    "    test_examples,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_builder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxBKH8iju6JY"
   },
   "source": [
    "## The Pytorch Model\n",
    "\n",
    "### Logistic Regression\n",
    "- Let's start by setting the hyper parameters of our yet-to-define model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DCkTrsnk7Qv"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters \n",
    "input_size = len(src_vocab)\n",
    "num_classes = len(tgt_vocab)\n",
    "epochs = 20\n",
    "learning_rate = 0.5\n",
    "log_interval = 100\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfEnY3P7whsg"
   },
   "source": [
    "- Our next step is to define the model. To do so, we can extend the `torch.nn.Module` class, which will alow us to reuse some of the internal structure that Pytorch has prepared. \n",
    "- We need to define the `__init__()` and `forward()` functions which will take care of initializing the parameters of our model and computing the outputs given an example, respectively.\n",
    "- As long as we use Pytorch objects and operations, we do not have to define the `backward()` function ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJyNgHq9lEkC"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PxUTcuZy3Uo"
   },
   "outputs": [],
   "source": [
    "nn.Linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fkEnHaXxrLS"
   },
   "source": [
    "- `nn.Linear()` offers us a shortcut for defining single layer neural networks of the form $y = Ax  + b$ , following all the good practices of parameter initialization.\n",
    "- Note that the API for `nn.Linear()` expects a tensor of size `(N_examples, n_features)`, where generally `N_features` indicates that we could pass a *mini-batch* of examples to the model at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGSvbyci89So"
   },
   "source": [
    "- Next, let's instantiate our model and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "RZKfvIcSO-68",
    "outputId": "56cb0d60-230a-4cab-d3f2-309d64c50721",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes)\n",
    "model = model.to(device=device)\n",
    "  \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bXlwPIK7iCi"
   },
   "source": [
    "Note that the parameters created by our Model object as just regular PyTorch Tensors wrapped in the `Parameter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "id": "ccWAqk1fPTQ1",
    "outputId": "7a916734-93d5-40f3-9ab6-fc8ec46068fa"
   },
   "outputs": [],
   "source": [
    "print(model.linear.weight)\n",
    "print(model.linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0C_mKfJi7vl-"
   },
   "source": [
    "Now we are going to set the adequate loss function for our problem, and prepare us to  use stochastic gradient descent as our training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93OzDnr66tFn"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28jM5tZS8ri8"
   },
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTUlMDeM8oxA"
   },
   "source": [
    "- `nn.CrossEntropy()` is a Pytorch efficient implementation of the cross entropy, which is the loss funtion of preference for multi-class classification. Note that it expects two inputs: a tensor of size `(N_examples, N_classes)` containing the logits (non-normalized probabilities) for each class on each instance, and the hard label for the batch of shape `(N_examples)`.\n",
    "- The function `model.parameters()` returns an iterable over the paramerets of the model, which are simply tensors wrapped in the special `nn.Parameter()` object.\n",
    "- The `torch.optim.SGD()` object receives our model parameters and the learning rate, and is in charge of simply updating these using the gradients and the update rule that we are familiar with: $\\theta \\leftarrow \\theta - \\alpha * \\nabla \\theta$, where $\\alpha$ is our learning rate and $\\theta$ symbolizes our model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D14xlrx67no"
   },
   "source": [
    "- Now we can train our model using all the components we've built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1016
    },
    "colab_type": "code",
    "id": "eo8LwG7-lISC",
    "outputId": "03bcdd7a-fab7-4af7-bbb0-76d244ae5c13"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    epoch_loss = 0\n",
    "    i = 0\n",
    "    \n",
    "    model.train()\n",
    "  \n",
    "    for train_batch in train_batches:\n",
    "        \n",
    "        # we move our data to PyTorch\n",
    "        # and to the GPU if necessary\n",
    "        train_batch.to_torch_(device)\n",
    "        \n",
    "        # make sure our gradients are 0 to start\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # call forward() to compute the \n",
    "        # outputs of the model given our examples\n",
    "        outputs = model(train_batch.src)\n",
    "        \n",
    "        # compute the loss and call backward()\n",
    "        # to compute gradients\n",
    "        loss = loss_function(outputs, train_batch.tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        # apply our learning rule using the gradients\n",
    "        # stored in the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predictions = outputs.max(1)\n",
    "        \n",
    "        correct = (predictions == train_batch.tgt).long().sum()\n",
    "        total = train_batch.tgt.size(0)\n",
    "        epoch_correct += correct.item()\n",
    "        epoch_total += total\n",
    "        epoch_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "    accuracy  = 100 * epoch_correct / epoch_total\n",
    "    \n",
    "    print('Epoch {}'.format(epoch))\n",
    "    print('Train Loss: {}'.format(epoch_loss / len(train_batches)))\n",
    "    print('Train Accuracy: {}'.format(accuracy))\n",
    "\n",
    "    test_epoch_correct = 0\n",
    "    test_epoch_total = 0\n",
    "    test_epoch_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for test_batch in test_batches:\n",
    "        \n",
    "        # we move our data to PyTorch\n",
    "        # and to the GPU if necessary\n",
    "        test_batch.to_torch_(device)\n",
    "        \n",
    "        # call forward() to compute the \n",
    "        # outputs of the model given our examples\n",
    "        outputs = model(test_batch.src)\n",
    "        \n",
    "        loss = loss_function(outputs, test_batch.tgt)\n",
    "        \n",
    "        _, predictions = outputs.max(1)\n",
    "        \n",
    "        correct = (predictions == test_batch.tgt).long().sum()\n",
    "        total = test_batch.tgt.size(0)\n",
    "        test_epoch_correct += correct.item()\n",
    "        test_epoch_total += total\n",
    "        test_epoch_loss += loss.item()\n",
    "\n",
    "    test_accuracy = 100 * test_epoch_correct / test_epoch_total\n",
    "\n",
    "    print('\\n---------------------')\n",
    "    print('Test Loss: {}'.format(test_epoch_loss / len(test_batches)))\n",
    "    print('Test Accuracy: {}'.format(test_accuracy))\n",
    "    print('---------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNQ8Wyl1lIPQ"
   },
   "outputs": [],
   "source": [
    "# save the trained model (a.k.a model parameters)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sequence_classification_log_reg.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
